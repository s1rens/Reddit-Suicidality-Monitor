{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "colab": {
      "name": "Project - Building Classifier.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s1rens/Reddit-Suicidality-Monitor/blob/main/Building_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dr7resaK3b1T"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing, metrics\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import spacy\n",
        "import pickle\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt \n",
        "import plotly.express as px\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Yy3XXso3b1Y"
      },
      "source": [
        "# preprocess dataset\n",
        "df = pd.read_csv('suicidality_dataset_raw_fixed.csv', header=0)\n",
        "df['postfixed'] = df['post'].str.lower()\n",
        "df['postfixed'].replace(to_replace='http\\S+', value='', inplace=True, regex=True) # remove URLs\n",
        "df['postfixed'].replace(to_replace='amp;\\S+', value='', inplace=True, regex=True) # remove formatting\n",
        "df['postfixed'].replace('\\\\n', '', inplace=True, regex=True)\n",
        "df['postfixed'].replace('\\\\n\\\\n', '', inplace=True, regex=True)\n",
        "df['goodlemma'] = df['postfixed'].apply(lambda x: \" \".join([y.lemma_ for y in nlp(x) if not y.is_stop and not y.is_punct])) # lemmatise, remove stop words and punctuation, remove 'http and 'com''\n",
        "tf = TfidfVectorizer(lowercase=True, stop_words='english', strip_accents='ascii', min_df=10)\n",
        "tf_df = tf.fit_transform(df['goodlemma'])\n",
        "selector = SelectKBest(chi2)\n",
        "selector.fit(tf_df, df['suicidal'])\n",
        "chi2_scores = pd.DataFrame(list(zip(tf.get_feature_names(), selector.scores_, selector.pvalues_)), columns=['word', 'score', 'p-value'])\n",
        "important_word_df = chi2_scores.sort_values(by=['score'], ascending=False)\n",
        "num_features = 3000 # number of most important features to keep\n",
        "important_word_df = important_word_df.iloc[:num_features] # Takes most important features\n",
        "print(important_word_df.head(10)) # prints top 10 most important features\n",
        "print(important_word_df.tail(10)) # prints least 10 most important features\n",
        "important_word_list = important_word_df['word'].tolist()\n",
        "df['goodlemma_most_important'] = df['goodlemma'].apply(lambda x: \" \".join([str(y) for y in nlp(x) if str(y) in important_word_list])) # remove all words that aren't in important_word_list\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYte0pf43b1a"
      },
      "source": [
        "# split data into train and test sets\n",
        "seed = 0\n",
        "train, test = train_test_split(df, test_size=0.2, shuffle=True, random_state=seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcD6YuP13b1b"
      },
      "source": [
        "# create train and test tfidf matrices\n",
        "train_tf = tf.fit_transform(train['goodlemma_most_important'])\n",
        "test_tf = tf.transform(test['goodlemma_most_important'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68RUa-Eq3b1b"
      },
      "source": [
        "# extract Xtrain, Xtest, ytrain, ytest\n",
        "Xtrain = pd.DataFrame(train_tf.toarray())\n",
        "ytrain = train.iloc[:, 4]\n",
        "Xtest = pd.DataFrame(test_tf.toarray())\n",
        "ytest = test.iloc[:, 4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhYrufH33b1c"
      },
      "source": [
        "# evaluation\n",
        "def fit_predict(model, Xtrain, ytrain, Xtest, ytest):\n",
        "    model.fit(Xtrain, ytrain)\n",
        "    y_hat = model.predict(Xtest)\n",
        "    print('Classification report for',model,':')\n",
        "    print(classification_report(ytest, y_hat))\n",
        "    print('Confusion matrix for',model,':')\n",
        "    print(metrics.confusion_matrix(ytest, y_hat))\n",
        "    return y_hat\n",
        "\n",
        "# logistic regression\n",
        "lr = LogisticRegression()\n",
        "lr_y_hat = fit_predict(lr, Xtrain, ytrain, Xtest, ytest)\n",
        "\n",
        "# SVM\n",
        "svm = SVC(kernel='linear')\n",
        "svm_y_hat = fit_predict(svm, Xtrain, ytrain, Xtest, ytest)\n",
        "\n",
        "# random forest\n",
        "rf = RandomForestClassifier()\n",
        "rf_y_hat = fit_predict(rf, Xtrain, ytrain, Xtest, ytest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qpDfy5x3b1d"
      },
      "source": [
        "# plot roc curves\n",
        "fig, ax = plt.subplots()\n",
        "fig.set_figwidth(8)\n",
        "fig.set_figheight(5)\n",
        "metrics.plot_roc_curve(lr, Xtest, ytest, ax=ax)\n",
        "metrics.plot_roc_curve(svm, Xtest, ytest, ax=ax)\n",
        "metrics.plot_roc_curve(rf, Xtest, ytest, ax=ax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZWBzgng3b1e"
      },
      "source": [
        "# plotting. This won't run on colab but it's not important. Just interactive version of the ROC curve above.\n",
        "# citation for plotting ROC/AUC code: https://towardsdatascience.com/an-understandable-guide-to-roc-curves-and-auc-and-why-and-when-to-use-them-92020bc4c5c1\n",
        "y_pred_proba = lr.predict_proba(Xtest)[:,1]\n",
        "fpr, tpr, thresh = metrics.roc_curve(ytest, y_pred_proba)\n",
        "roc_df = pd.DataFrame(zip(fpr, tpr, thresh),columns = [\"FPR\",\"TPR\",\"Threshold\"])\n",
        "display(roc_df)\n",
        "fig = px.area(roc_df, x='FPR', y='TPR', hover_data = ['Threshold'])\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaGgBO4J3b1f"
      },
      "source": [
        "# save models\n",
        "with open('model_logistic_regression_3000.pkl', 'wb') as file:\n",
        "    pickle.dump(lr, file)\n",
        "\n",
        "with open('model_svm_3000.pkl', 'wb') as file:\n",
        "    pickle.dump(svm, file)\n",
        "\n",
        "with open('model_random_forest_3000.pkl', 'wb') as file:\n",
        "    pickle.dump(rf, file)\n",
        "    \n",
        "with open('tfidf_3000.pkl', 'wb') as file:\n",
        "    pickle.dump(tf, file)\n",
        "    \n",
        "with open('important_words_3000.txt', 'w') as file:\n",
        "    file.write('\\n'.join(important_word_list))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}